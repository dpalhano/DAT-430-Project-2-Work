{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e4eb52-6d63-4eba-b94c-c7286de0dcdc",
   "metadata": {},
   "source": [
    "# DAT-430 Module  Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed57ae",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "There are two data files for HR attrition data. Data dictionaries for the two files are provided in the instructions. In this project, you will merge the two files into one data set and create two predictive models. You will then analyze model outputs using a Power BI dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b690e",
   "metadata": {},
   "source": [
    "### Load Python Libraries\n",
    "\n",
    "Run the following step to load Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786eb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries you will use in this project\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf49de1-2151-4900-9e1b-d8566c8e21cd",
   "metadata": {},
   "source": [
    "### Step 1: Merge the two data files\n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7323a26-4de9-4313-8874-5d5e94e6420c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR 1 data:  rows=1470, cols=10\n",
      "HR 2 data:  rows=1470, cols=21\n",
      "Merged data:  rows=1470, cols=30\n"
     ]
    }
   ],
   "source": [
    "# 1-1 Upload first data set\n",
    "hr1_df = pd.read_csv('HRData1.csv')\n",
    "hr1_shape = hr1_df.shape\n",
    "hr1_data_rows, hr1_data_cols = hr1_shape[0], hr1_shape[1]\n",
    "\n",
    "# 1-2 Upload second data set\n",
    "hr2_df = pd.read_csv('HRData2.csv')\n",
    "hr2_shape = hr2_df.shape\n",
    "hr2_data_rows, hr2_data_cols = hr2_shape[0], hr2_shape[1]\n",
    "\n",
    "# 1-3 Merge the two data sets\n",
    "hr_merged_df = pd.merge(hr1_df, hr2_df, how='inner', on='EmployeeNumber')\n",
    "hr_merged_shape = hr_merged_df.shape\n",
    "hr_merged_data_rows, hr_merged_data_cols = hr_merged_shape[0], hr_merged_shape[1]\n",
    "\n",
    "\n",
    "print(f'HR 1 data:  rows={hr1_data_rows}, cols={hr1_data_cols}')\n",
    "print(f'HR 2 data:  rows={hr2_data_rows}, cols={hr2_data_cols}')\n",
    "print(f'Merged data:  rows={hr_merged_data_rows}, cols={hr_merged_data_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dd11f-d539-461a-b633-4bdcbe1ce797",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering\n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9ceabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Data: rows = 1470, cols = 19\n"
     ]
    }
   ],
   "source": [
    "# 2-1 Create High Income feature\n",
    "hr_merged_df['high_income'] = 0\n",
    "hr_merged_df.loc[hr_merged_df['MonthlyIncome'] >= 7000, 'high_income'] = 1\n",
    "\n",
    "# 2-2 Create Tenured feature\n",
    "hr_merged_df['Tenured'] = 0\n",
    "hr_merged_df.loc[hr_merged_df['YearsAtCompany'] >= 11, 'Tenured'] = 1\n",
    "\n",
    "# 2-3 Copy Data Set with a Subset of Features \n",
    "data_model_df = hr_merged_df[['Age', 'Attrition', 'Department', 'Education', 'Gender', \n",
    "                              'high_income', 'JobRole', 'JobSatisfaction', 'MaritalStatus', \n",
    "                              'MonthlyIncome', 'NumCompaniesWorked', 'PerformanceRating', \n",
    "                              'StockOptionLevel', 'Tenured', 'TotalWorkingYears', 'TrainingTimesLastYear', \n",
    "                              'YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager']].copy()\n",
    "data_model_shape = data_model_df.shape\n",
    "data_rows, data_cols = data_model_shape[0], data_model_shape[1]\n",
    "print(f'Feature Engineering Data: rows = {data_rows}, cols = {data_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4517df",
   "metadata": {},
   "source": [
    "### Step 3: One Hot Encode Categorical Features \n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cfad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Data: rows = 1470, cols = 32\n"
     ]
    }
   ],
   "source": [
    "# 3-1 Create a function to One Hot Encode Categorical variables\n",
    "def onehot_encoder(df, column_names):\n",
    "    \"\"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns_names (list): A list of column names to encode.\n",
    "                    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with specified columns encoded.\n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    one_hot_encoded = encoder.fit_transform(df[column_names])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(column_names))\n",
    "    df_encoded = pd.concat([df, one_hot_df], axis = 1)\n",
    "    df_encoded = df_encoded.drop(column_names, axis = 1)\n",
    "    \n",
    "    return df_encoded\n",
    "    \n",
    "# 3-2 Run the function on Categorical variables in the data set\n",
    "data_df = onehot_encoder(data_model_df, ['Gender', 'JobRole', 'Department', 'MaritalStatus'])\n",
    "data_shape = data_df.shape\n",
    "data_rows, data_cols = data_shape[0], data_shape[1]\n",
    "print(f'Encoded Data: rows = {data_rows}, cols = {data_cols}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dad36",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Training and Testing Sets \n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118f2f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  X = (1029, 31), y = (1029,)\n",
      "Testing Set:  X = (441, 31), y = (441,)\n"
     ]
    }
   ],
   "source": [
    "# 4-1 Split data into Feature and Target vectors\n",
    "target = 'Attrition'\n",
    "X = data_df.drop(target, axis = 1)\n",
    "y = data_df[target]\n",
    "\n",
    "# 4-2 Create Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 67)\n",
    "\n",
    "print(f'Training Set:  X = {X_train.shape}, y = {y_train.shape}')\n",
    "print(f'Testing Set:  X = {X_test.shape}, y = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fd3f2",
   "metadata": {},
   "source": [
    "### Step 5: Random Forest Classification Model \n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38eb8a30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set metrics\n",
      "accuracy = 0.97\n",
      "precision = 0.99\n",
      "recall = 0.92\n",
      "\n",
      "test set metrics\n",
      "accuracy = 0.86\n",
      "precision = 0.82\n",
      "recall = 0.56\n"
     ]
    }
   ],
   "source": [
    "# 5-1 Run Random Forest Classifier to predict Attrition\n",
    "model = RandomForestClassifier(random_state = 67)\n",
    "\n",
    "## Hyperparameter Tuning & Fit\n",
    "param_grid = {  \"n_estimators\"      : [250, 300],\n",
    "                \"max_depth\"         : [5, 10],\n",
    "                \"bootstrap\": [True, False],\n",
    "                \"class_weight\": ['balanced']}\n",
    "rf_grid_search = GridSearchCV(model, param_grid, n_jobs = -1, cv = 2)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 5-2 Predict on the Train and Test Sets\n",
    "y_rf_train_pred = rf_grid_search.predict(X_train)\n",
    "y_rf_test_pred = rf_grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "# 5-3 Report Metrics\n",
    "# accuracy_score, precision_score, recall_score\n",
    "train_accuracy = accuracy_score(y_train, y_rf_train_pred)\n",
    "train_precision = precision_score(y_train, y_rf_train_pred)\n",
    "train_recall = recall_score(y_train, y_rf_train_pred)\n",
    "print(f\"\\ntrain set metrics\")\n",
    "print(f\"accuracy = {round(train_accuracy, 2)}\")\n",
    "print(f\"precision = {round(train_precision, 2)}\")\n",
    "print(f\"recall = {round(train_recall, 2)}\")\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_rf_test_pred)\n",
    "test_precision = precision_score(y_test, y_rf_test_pred)\n",
    "test_recall = recall_score(y_test, y_rf_test_pred)\n",
    "print(f\"\\ntest set metrics\")\n",
    "print(f\"accuracy = {round(test_accuracy, 2)}\")\n",
    "print(f\"precision = {round(test_precision, 2)}\")\n",
    "print(f\"recall = {round(test_recall, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32686683",
   "metadata": {},
   "source": [
    "### Step 6: Ada Boost Classification Model \n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5fb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set metrics\n",
      "accuracy = 0.86\n",
      "precision = 0.92\n",
      "recall = 0.56\n",
      "\n",
      "test set metrics\n",
      "accuracy = 0.85\n",
      "precision = 0.82\n",
      "recall = 0.5\n"
     ]
    }
   ],
   "source": [
    "# 6-1 Run Ada Boost Classifier to predict Attrition\n",
    "model = AdaBoostClassifier(random_state = 67)\n",
    "\n",
    "## Hyperparameter Tuning & Fit\n",
    "param_grid = {  \"n_estimators\"   : [250, 300],\n",
    "                \"learning_rate\"  : [0.1, 0.01, 0.001]}\n",
    "\n",
    "ada_grid_search = GridSearchCV(model, param_grid, n_jobs=-1, cv=2)\n",
    "ada_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6-2 Predict on the Train and Test Sets\n",
    "y_ada_train_pred = ada_grid_search.predict(X_train)\n",
    "y_ada_test_pred = ada_grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "# 6-3 Report Metrics\n",
    "# accuracy_score, precision_score, recall_score\n",
    "train_accuracy = accuracy_score(y_train, y_ada_train_pred)\n",
    "train_precision = precision_score(y_train, y_ada_train_pred)\n",
    "train_recall = recall_score(y_train, y_ada_train_pred)\n",
    "print(f\"\\ntrain set metrics\")\n",
    "print(f\"accuracy = {round(train_accuracy, 2)}\")\n",
    "print(f\"precision = {round(train_precision, 2)}\")\n",
    "print(f\"recall = {round(train_recall, 2)}\")\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_ada_test_pred)\n",
    "test_precision = precision_score(y_test, y_ada_test_pred)\n",
    "test_recall = recall_score(y_test, y_ada_test_pred)\n",
    "print(f\"\\ntest set metrics\")\n",
    "print(f\"accuracy = {round(test_accuracy, 2)}\")\n",
    "print(f\"precision = {round(test_precision, 2)}\")\n",
    "print(f\"recall = {round(test_recall, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59e415",
   "metadata": {},
   "source": [
    "### Step 7: Merge Predictions with Data Sets and Save\n",
    "\n",
    "Follow instructions provided for this step and write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce42c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1 Merge and Save Outputs\n",
    "## Merge train set predictions with the original train data\n",
    "rf_train_pred_df = pd.DataFrame({'RF_Attrition_Pred': y_rf_train_pred}, index=X_train.index)\n",
    "ada_train_pred_df = pd.DataFrame({'ADA_Attrition_Pred': y_ada_train_pred}, index=X_train.index)\n",
    "\n",
    "train_actual_df = pd.DataFrame({'Attrition': y_train}, index=X_train.index)\n",
    "train_merged_df = pd.concat([X_train, train_actual_df, rf_train_pred_df, ada_train_pred_df], axis=1)\n",
    "train_merged_df.to_csv('predictions_train.csv', index=False)\n",
    "\n",
    "\n",
    "## Merge test set predictions with the original test data\n",
    "rf_test_pred_df = pd.DataFrame({'Attrition_Pred': y_rf_test_pred}, index=X_test.index)\n",
    "ada_test_pred_df = pd.DataFrame({'ADA_Attrition_Pred': y_ada_test_pred}, index=X_test.index)\n",
    "\n",
    "test_actual_df = pd.DataFrame({'Attrition': y_test}, index=X_test.index)\n",
    "test_merged_df = pd.concat([X_test, test_actual_df, rf_test_pred_df, ada_test_pred_df], axis=1)\n",
    "test_merged_df.to_csv('predictions_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f655ba",
   "metadata": {},
   "source": [
    "### Step 8: Power BI Dashboard\n",
    "\n",
    "Continue analysis in Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcd9ae",
   "metadata": {},
   "source": [
    "## End of Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
